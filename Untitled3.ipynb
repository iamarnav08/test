{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZY8M6RZY-e7",
        "outputId": "5da510ce-7391-4ae8-b7a1-2b0b4f813cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to import the datasets to be used as labels."
      ],
      "metadata": {
        "id": "YxM9fhB6kPAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define paths\n",
        "BASE_DIR = \"/content/drive/My Drive/Precog Task/task1/same_on_same\"\n",
        "\n",
        "# EASY_IMAGES_DIR = os.path.join(BASE_DIR, \"easy_variations\")\n",
        "# HARD_IMAGES_DIR = os.path.join(BASE_DIR, \"hard_variations\")\n",
        "EASY_IMAGES_DIR = BASE_DIR\n",
        "HARD_IMAGES_DIR = BASE_DIR\n",
        "\n",
        "EASY_LABELS_CSV = os.path.join(BASE_DIR, \"easy_variations.csv\")\n",
        "HARD_LABELS_CSV = os.path.join(BASE_DIR, \"hard_variations.csv\")\n",
        "print(EASY_IMAGES_DIR)\n",
        "print(HARD_IMAGES_DIR)\n",
        "print(EASY_LABELS_CSV)\n",
        "print(HARD_LABELS_CSV)\n"
      ],
      "metadata": {
        "id": "IJcRS91skUJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9eea0b-0f16-4022-b84f-98b541c081c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Precog Task/task1/same_on_same\n",
            "/content/drive/My Drive/Precog Task/task1/same_on_same\n",
            "/content/drive/My Drive/Precog Task/task1/same_on_same/easy_variations.csv\n",
            "/content/drive/My Drive/Precog Task/task1/same_on_same/hard_variations.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have loaded the dataset, we will make the dataset.\n"
      ],
      "metadata": {
        "id": "61XGcGQd5eit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation function will be put here.\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1,1] for stable training\n",
        "])"
      ],
      "metadata": {
        "id": "Ub7UWfjLAXIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "# Read all words from CSV files\n",
        "easy_df = pd.read_csv(EASY_LABELS_CSV)\n",
        "hard_df = pd.read_csv(HARD_LABELS_CSV)\n",
        "all_words = np.concatenate([easy_df['Word'].values, hard_df['Word'].values])\n",
        "label_encoder.fit(all_words)\n",
        "\n",
        "class WordImageDataset(Dataset):\n",
        "    def __init__(self, csv_file, image_folder, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)  # Read CSV file\n",
        "        print(csv_file)\n",
        "        # print(image_folder)\n",
        "        # for filename in os.listdir(image_folder):\n",
        "        #     print(filename)\n",
        "        # for index, row in self.data.iterrows():\n",
        "        #     print(row['Image_Path'])\n",
        "        self.image_folder = image_folder  # Folder where images are stored\n",
        "        self.transform = transform  # Image transformations (optional)\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)  # Returns number of images in dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_folder, self.data.iloc[idx, 1])  # Image filename\n",
        "        Image_Value = Image.open(img_name).convert(\"RGB\")  # Convert to RGB\n",
        "        Word = self.data.iloc[idx, 0]  # Word label\n",
        "        Image_Path = os.path.join(self.image_folder, img_name)\n",
        "\n",
        "        # if self.transform:\n",
        "        #     image = self.transform(image)\n",
        "\n",
        "        # return image, label\n",
        "        try:\n",
        "            image = Image.open(Image_Path).convert(\"RGB\")\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "\n",
        "\n",
        "            # return Word, image\n",
        "            if self.label_encoder:\n",
        "                label = self.label_encoder.transform([Word])[0]\n",
        "                return image, torch.tensor(label, dtype=torch.long)\n",
        "            return Word, image\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error loading image {Image_Path}: {str(e)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "easy_dataset = WordImageDataset(EASY_LABELS_CSV, EASY_IMAGES_DIR, transform)\n",
        "hard_dataset = WordImageDataset(HARD_LABELS_CSV, HARD_IMAGES_DIR, transform)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eErDQrI95mSY",
        "outputId": "f9ccab0a-c962-414b-a94d-3c8bd5a97781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Precog Task/task1/same_on_same/easy_variations.csv\n",
            "/content/drive/My Drive/Precog Task/task1/same_on_same/hard_variations.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will split the dataset we have created into Training and Testing sets.\n",
        "There are 1000 images per easy and hard datasets. We will take 20% for the testing dataset.\n"
      ],
      "metadata": {
        "id": "X5RnVT0V7KQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Defining train-test partition (80% train, 20% test)\n",
        "train_size = int(0.8 * len(easy_dataset))\n",
        "test_size = len(easy_dataset) - train_size\n",
        "\n",
        "easy_train, easy_test = random_split(easy_dataset, [train_size, test_size])\n",
        "hard_train, hard_test = random_split(hard_dataset, [train_size, test_size])\n",
        "\n",
        "\n",
        "# Creating DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(easy_train + hard_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(easy_test + hard_test, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "97p82E9N7OhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch_idx, (target, data) in enumerate(train_loader):\n",
        "#     print(f\"Batch Index: {batch_idx}\")\n",
        "#     print(f\"Data: {data}\")  # Print the data\n",
        "#     print(f\"Target (Labels): {target}\")  # Print the labels\n",
        "\n",
        "#     # You might want to limit the number of batches printed for brevity\n",
        "#     if batch_idx == 2:  # Print only 3 batches\n",
        "#         break"
      ],
      "metadata": {
        "id": "aIvKH6JeJU0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to define the architecture of the CNN.\n"
      ],
      "metadata": {
        "id": "JEriSX8oGxHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)  # (32, 100, 400)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # (64, 100, 400)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Downsamples (50x200)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # (128, 50, 200)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)  # (256, 50, 200)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Downsamples (25x100)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(256 * 25 * 100, 512)  # Flatten and connect to 512 neurons\n",
        "        self.dropout = nn.Dropout(0.5)  # Dropout for regularization\n",
        "        self.fc2 = nn.Linear(512, num_classes)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool2(F.relu(self.conv4(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.fc2(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = CNNClassifier(num_classes=100)\n"
      ],
      "metadata": {
        "id": "a6MWwdviG0H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "8QS25jx9OJyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_loader:\n",
        "    print(\"Sample Labels:\", labels[:10])\n",
        "    break  # Print once and exit loop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkDQruTUmHgN",
        "outputId": "6afa751b-2b8b-496c-e55b-7487e55f9874"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Labels: tensor([29, 16, 71,  2, 86, 79, 43, 19, 18, 74])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to train the model."
      ],
      "metadata": {
        "id": "oUeAJ23RWL_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_epochs = 10\n",
        "total_steps = len(train_loader)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(num_of_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        # Move to device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_of_epochs}], Step [{batch_idx+1}/{total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Print epoch stats\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_of_epochs}], Average Loss: {epoch_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKNA_7jtWOlE",
        "outputId": "2b3110b8-7014-4ce6-b41a-b9e084520f83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [1/50], Loss: 4.6038\n",
            "Epoch [1/10], Step [11/50], Loss: 4.6152\n",
            "Epoch [1/10], Step [21/50], Loss: 4.6030\n",
            "Epoch [1/10], Step [31/50], Loss: 4.6030\n",
            "Epoch [1/10], Step [41/50], Loss: 4.6033\n",
            "Epoch [1/10], Average Loss: 5.0939\n",
            "Epoch [2/10], Step [1/50], Loss: 4.6070\n",
            "Epoch [2/10], Step [11/50], Loss: 4.6018\n",
            "Epoch [2/10], Step [21/50], Loss: 4.6022\n",
            "Epoch [2/10], Step [31/50], Loss: 4.6046\n",
            "Epoch [2/10], Step [41/50], Loss: 4.6062\n",
            "Epoch [2/10], Average Loss: 4.6049\n",
            "Epoch [3/10], Step [1/50], Loss: 4.6040\n",
            "Epoch [3/10], Step [11/50], Loss: 4.5984\n",
            "Epoch [3/10], Step [21/50], Loss: 4.6009\n",
            "Epoch [3/10], Step [31/50], Loss: 4.6026\n",
            "Epoch [3/10], Step [41/50], Loss: 4.6112\n",
            "Epoch [3/10], Average Loss: 4.6047\n",
            "Epoch [4/10], Step [1/50], Loss: 4.6082\n",
            "Epoch [4/10], Step [11/50], Loss: 4.6093\n",
            "Epoch [4/10], Step [21/50], Loss: 4.6000\n",
            "Epoch [4/10], Step [31/50], Loss: 4.6015\n",
            "Epoch [4/10], Step [41/50], Loss: 4.6082\n",
            "Epoch [4/10], Average Loss: 4.6042\n",
            "Epoch [5/10], Step [1/50], Loss: 4.6072\n",
            "Epoch [5/10], Step [11/50], Loss: 4.6000\n",
            "Epoch [5/10], Step [21/50], Loss: 4.5942\n",
            "Epoch [5/10], Step [31/50], Loss: 4.5982\n",
            "Epoch [5/10], Step [41/50], Loss: 4.6019\n",
            "Epoch [5/10], Average Loss: 4.6038\n",
            "Epoch [6/10], Step [1/50], Loss: 4.5979\n",
            "Epoch [6/10], Step [11/50], Loss: 4.5953\n",
            "Epoch [6/10], Step [21/50], Loss: 4.6123\n",
            "Epoch [6/10], Step [31/50], Loss: 4.5947\n",
            "Epoch [6/10], Step [41/50], Loss: 4.6026\n",
            "Epoch [6/10], Average Loss: 4.6034\n",
            "Epoch [7/10], Step [1/50], Loss: 4.6098\n",
            "Epoch [7/10], Step [11/50], Loss: 4.6085\n",
            "Epoch [7/10], Step [21/50], Loss: 4.6200\n",
            "Epoch [7/10], Step [31/50], Loss: 4.5960\n",
            "Epoch [7/10], Step [41/50], Loss: 4.6059\n",
            "Epoch [7/10], Average Loss: 4.6033\n",
            "Epoch [8/10], Step [1/50], Loss: 4.5991\n",
            "Epoch [8/10], Step [11/50], Loss: 4.5951\n",
            "Epoch [8/10], Step [21/50], Loss: 4.6048\n",
            "Epoch [8/10], Step [31/50], Loss: 4.5973\n",
            "Epoch [8/10], Step [41/50], Loss: 4.6079\n",
            "Epoch [8/10], Average Loss: 4.6029\n",
            "Epoch [9/10], Step [1/50], Loss: 4.6131\n",
            "Epoch [9/10], Step [11/50], Loss: 4.6011\n",
            "Epoch [9/10], Step [21/50], Loss: 4.6060\n",
            "Epoch [9/10], Step [31/50], Loss: 4.5923\n",
            "Epoch [9/10], Step [41/50], Loss: 4.6134\n",
            "Epoch [9/10], Average Loss: 4.6028\n",
            "Epoch [10/10], Step [1/50], Loss: 4.6024\n",
            "Epoch [10/10], Step [11/50], Loss: 4.5959\n",
            "Epoch [10/10], Step [21/50], Loss: 4.6046\n",
            "Epoch [10/10], Step [31/50], Loss: 4.6158\n",
            "Epoch [10/10], Step [41/50], Loss: 4.5902\n",
            "Epoch [10/10], Average Loss: 4.6022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing phase."
      ],
      "metadata": {
        "id": "4TPa6eSb-pci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model\n",
        "torch.save(model.state_dict(), \"cnn_model.pth\")"
      ],
      "metadata": {
        "id": "0uScyG1V6fW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() #Setting to eval mode\n",
        "right=0\n",
        "count=0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for images, labels in test_loader:\n",
        "    iamges, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    count += labels.size(0)\n",
        "    # right += (predicted == labels).sum().item()\n",
        "\n",
        "    print(right)\n",
        "\n",
        "accuracy = 100 * right/count\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "cAFNWGNY-rqy",
        "outputId": "3515798c-8a28-4cd3-97b9-f1bc1c8a655d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Boolean value of Tensor with more than one value is ambiguous",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-0d00d989aea1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# right += (predicted == labels).sum().item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m        \u001b[0mright\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNNClassifier(num_classes=100)  # Make sure to define CNNClassifier in the notebook\n",
        "\n",
        "# Load model weights\n",
        "model.load_state_dict(torch.load(\"/content/drive/My Drive/cnn_model.pth\", weights_only=True))\n",
        "model.to(device)  # Move model to GPU/CPU\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeLWZylVjNUA",
        "outputId": "6aa0736d-1a3d-49a7-fb69-e7b221928d77"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNClassifier(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=640000, out_features=512, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc2): Linear(in_features=512, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "right = 0\n",
        "count = 0\n",
        "\n",
        "with torch.no_grad():  # No gradients needed for testing\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move to device\n",
        "\n",
        "        outputs = model(images)  # Forward pass\n",
        "        _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
        "\n",
        "        count += labels.size(0)\n",
        "        right += (predicted == labels).sum().item()  # Count correct predictions\n",
        "\n",
        "        # Print actual vs predicted labels\n",
        "        print(f\"Actual: {labels.tolist()}, Predicted: {predicted.tolist()}\")\n",
        "\n",
        "# Compute and print final accuracy\n",
        "accuracy = 100 * right / count\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJXWkmy88nPQ",
        "outputId": "6ccd764b-52ac-426d-fbd2-cda924ba394c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual: [16, 53, 46, 81, 58, 5, 4, 94, 46, 87, 75, 13, 35, 44, 4, 45, 55, 62, 70, 63, 17, 58, 96, 58, 27, 40, 58, 75, 24, 63, 52, 38], Predicted: [93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93]\n",
            "Actual: [31, 80, 94, 0, 94, 17, 87, 86, 18, 0, 74, 96, 22, 84, 38, 79, 2, 35, 4, 35, 75, 52, 94, 50, 15, 58, 15, 63, 24, 55, 73, 45], Predicted: [93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93]\n",
            "Actual: [94, 79, 87, 33, 49, 81, 83, 99, 70, 33, 2, 44, 64, 53, 15, 17, 33, 35, 42, 49, 83, 54, 15, 38, 88, 16, 86, 51, 83, 84, 88, 52], Predicted: [93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93]\n",
            "Actual: [54, 64, 15, 24, 79, 96, 15, 54, 33, 53, 94, 44, 84, 24, 73, 50, 87, 45, 16, 96, 62, 38, 80, 33, 5, 96, 84, 83, 0, 0, 63, 81], Predicted: [93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93]\n",
            "Actual: [4, 73, 84, 6, 62, 2, 31, 62, 86, 74, 0, 35, 35, 6, 16, 50, 43, 2, 4, 63, 33, 55, 62, 35, 53, 33, 84, 3, 33, 46, 51, 96], Predicted: [93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93]\n",
            "Actual: [54, 31, 58, 94, 3, 40, 58, 94, 18, 6, 27, 46, 51, 51, 74, 31, 27, 54, 96, 53, 88, 16, 88, 75, 33, 22, 88, 0, 16, 70, 51, 16], Predicted: [93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93]\n",
            "Actual: [49, 42, 45, 75, 74, 49, 43, 96, 7, 76, 34, 36, 47, 98, 59, 28, 41, 61, 11, 78, 95, 26, 82, 29, 39, 95, 48, 66, 65, 12, 12, 56], Predicted: [93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93]\n",
            "Actual: [66, 37, 23, 57, 85, 10, 9, 98, 37, 36, 10, 77, 85, 85, 61, 36, 90, 20, 56, 7, 57, 48, 97, 61, 97, 85, 61, 28, 65, 25, 34, 19], Predicted: [93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93]\n",
            "Actual: [60, 68, 57, 14, 95, 59, 20, 61, 10, 85, 23, 37, 82, 91, 66, 68, 71, 82, 76, 65, 59, 60, 93, 71, 57, 78, 28, 47, 56, 37, 95, 92], Predicted: [93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93]\n",
            "Actual: [30, 57, 65, 89, 23, 57, 36, 32, 89, 67, 76, 89, 32, 23, 10, 10, 71, 23, 92, 72, 85, 76, 78, 67, 66, 30, 76, 90, 20, 36, 39, 78], Predicted: [93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93]\n",
            "Actual: [66, 19, 21, 91, 29, 91, 98, 77, 69, 14, 23, 12, 47, 25, 25, 89, 48, 21, 10, 47, 68, 28, 9, 67, 11, 71, 77, 69, 93, 37, 77, 93], Predicted: [93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93]\n",
            "Actual: [59, 71, 8, 1, 12, 11, 30, 29, 25, 92, 76, 89, 72, 23, 34, 89, 12, 85, 10, 77, 92, 69, 97, 56, 39, 85, 90, 32, 29, 66, 92, 32], Predicted: [93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93]\n",
            "Actual: [37, 91, 12, 91, 47, 29, 9, 37, 36, 12, 39, 39, 60, 72, 11, 37], Predicted: [93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93]\n",
            "Test Accuracy: 0.75%\n"
          ]
        }
      ]
    }
  ]
}